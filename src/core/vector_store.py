# src/core/vector_store.py

import os
import logging
from typing import List, Dict, Any, Optional
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import PyPDF2
from pathlib import Path
import json
from datetime import datetime
import hashlib

logger = logging.getLogger(__name__)

class VectorStore:
    def __init__(self, persist_directory: str = "chroma_db"):
        """ChromaDB tabanlƒ± vekt√∂r deposu ba≈ülatƒ±r"""
        self.persist_directory = persist_directory
        
        # ChromaDB istemcisini ba≈ülat
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # Sentence transformer modelini y√ºkle
        self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        # Koleksiyonu al veya olu≈ütur
        self.collection = self.client.get_or_create_collection(
            name="pdf_documents",
            metadata={"hnsw:space": "cosine"}
        )
        
        logger.info(f"‚úÖ VectorStore ba≈ülatƒ±ldƒ±. Koleksiyon: {self.collection.count()} dok√ºman")

    def extract_text_from_pdf(self, pdf_file) -> str:
        """PDF dosyasƒ±ndan metin √ßƒ±karƒ±r"""
        try:
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    page_text = page.extract_text()
                    text += f"\n--- Sayfa {page_num + 1} ---\n{page_text}"
                except Exception as e:
                    logger.warning(f"Sayfa {page_num + 1} okunamadƒ±: {e}")
                    continue
            
            return text.strip()
        except Exception as e:
            logger.error(f"‚ùå PDF okuma hatasƒ±: {e}")
            return ""

    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """Metni par√ßalara b√∂ler"""
        if not text:
            return []
        
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            
            # Kelime sƒ±nƒ±rƒ±nda kes
            if end < text_length:
                last_space = text.rfind(' ', start, end)
                if last_space > start:
                    end = last_space
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
            if start <= 0:
                start = end
        
        return chunks

    def add_pdf_document(self, pdf_file, filename: str, metadata: Optional[Dict] = None) -> bool:
        """PDF dosyasƒ±nƒ± vekt√∂r deposuna ekler"""
        try:
            # PDF'den metin √ßƒ±kar
            text = self.extract_text_from_pdf(pdf_file)
            if not text:
                logger.error(f"‚ùå PDF'den metin √ßƒ±karƒ±lamadƒ±: {filename}")
                return False
            
            # Dosya hash'i olu≈ütur (tekrar kontrol i√ßin)
            file_hash = hashlib.md5(text.encode()).hexdigest()
            
            # Aynƒ± dok√ºman daha √∂nce eklenmi≈ü mi kontrol et
            existing = self.collection.get(
                where={"file_hash": file_hash}
            )
            
            if existing['ids']:
                logger.info(f"üìÑ Dok√ºman zaten mevcut: {filename}")
                return True
            
            # Metni par√ßalara b√∂l
            chunks = self.chunk_text(text)
            if not chunks:
                logger.error(f"‚ùå Metin par√ßalanmadƒ±: {filename}")
                return False
            
            # Metadata hazƒ±rla
            doc_metadata = {
                "filename": filename,
                "file_hash": file_hash,
                "upload_date": datetime.now().isoformat(),
                "chunk_count": len(chunks),
                **(metadata or {})
            }
            
            # Her par√ßa i√ßin ID ve metadata olu≈ütur
            ids = []
            metadatas = []
            documents = []
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{file_hash}_{i}"
                chunk_metadata = {
                    **doc_metadata,
                    "chunk_index": i,
                    "chunk_id": chunk_id
                }
                
                ids.append(chunk_id)
                metadatas.append(chunk_metadata)
                documents.append(chunk)
            
            # Vekt√∂r deposuna ekle
            self.collection.add(
                ids=ids,
                documents=documents,
                metadatas=metadatas
            )
            
            logger.info(f"‚úÖ PDF eklendi: {filename} ({len(chunks)} par√ßa)")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå PDF ekleme hatasƒ±: {e}")
            return False

    def search_similar(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Sorguya benzer dok√ºmanlarƒ± arar"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                include=["documents", "metadatas", "distances"]
            )
            
            if not results['documents'] or not results['documents'][0]:
                return []
            
            search_results = []
            for i in range(len(results['documents'][0])):
                result = {
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "similarity": 1 - results['distances'][0][i],  # Cosine distance'i similarity'ye √ßevir
                }
                search_results.append(result)
            
            logger.info(f"üîç Arama tamamlandƒ±: {len(search_results)} sonu√ß bulundu")
            return search_results
            
        except Exception as e:
            logger.error(f"‚ùå Arama hatasƒ±: {e}")
            return []

    def get_all_documents(self) -> List[Dict[str, Any]]:
        """T√ºm dok√ºmanlarƒ± listeler"""
        try:
            results = self.collection.get(
                include=["metadatas"]
            )
            
            # Dosya bazƒ±nda grupla
            documents = {}
            for metadata in results['metadatas']:
                filename = metadata.get('filename', 'Unknown')
                if filename not in documents:
                    documents[filename] = {
                        "filename": filename,
                        "upload_date": metadata.get('upload_date'),
                        "chunk_count": metadata.get('chunk_count', 0),
                        "file_hash": metadata.get('file_hash')
                    }
            
            return list(documents.values())
            
        except Exception as e:
            logger.error(f"‚ùå Dok√ºman listeleme hatasƒ±: {e}")
            return []

    def delete_document(self, file_hash: str) -> bool:
        """Dok√ºmanƒ± siler"""
        try:
            # Dok√ºmanƒ±n par√ßalarƒ±nƒ± bul
            results = self.collection.get(
                where={"file_hash": file_hash}
            )
            
            if not results['ids']:
                logger.warning(f"‚ö†Ô∏è Silinecek dok√ºman bulunamadƒ±: {file_hash}")
                return False
            
            # T√ºm par√ßalarƒ± sil
            self.collection.delete(ids=results['ids'])
            
            logger.info(f"üóëÔ∏è Dok√ºman silindi: {file_hash}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Dok√ºman silme hatasƒ±: {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """Vekt√∂r deposu istatistiklerini d√∂ner"""
        try:
            total_chunks = self.collection.count()
            documents = self.get_all_documents()
            
            return {
                "total_documents": len(documents),
                "total_chunks": total_chunks,
                "average_chunks_per_doc": total_chunks / len(documents) if documents else 0,
                "documents": documents
            }
            
        except Exception as e:
            logger.error(f"‚ùå ƒ∞statistik alma hatasƒ±: {e}")
            return {"error": str(e)}