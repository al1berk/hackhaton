from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pathlib import Path
import json
import uvicorn
import asyncio
from datetime import datetime
from langgraph_dialog import AsyncLangGraphDialog
from config import Config

app = FastAPI(
    title="LangGraph + CrewAI Async Multi-Agent Chat",
    description="Gemini ve CrewAI ile asenkron gÃ¼Ã§lendirilmiÅŸ akÄ±llÄ± araÅŸtÄ±rma asistanÄ±",
    version="2.1.0"
)

# Static directory setup
static_dir = Path("static")
static_dir.mkdir(exist_ok=True)

# Research data directory
research_dir = Path("research_data")
research_dir.mkdir(exist_ok=True)

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

# Active dialog instances
dialog_instances = {}
connection_stats = {
    "total_connections": 0,
    "active_connections": 0,
    "messages_processed": 0,
    "crew_researches": 0,
    "async_operations": 0,
    "start_time": datetime.utcnow()
}

@app.on_event("startup")
async def startup_event():
    """Application startup tasks"""
    print("ğŸš€ Async LangGraph + CrewAI Multi-Agent System baÅŸlatÄ±lÄ±yor...")
    
    try:
        # Config doÄŸrulamasÄ±
        Config.validate_config()
        print("âœ… API anahtarlarÄ± doÄŸrulandÄ±")
        
        # Test async connections
        test_dialog = AsyncLangGraphDialog()
        print("âœ… Async LangGraph + CrewAI sistemi hazÄ±r")
        
    except Exception as e:
        print(f"âŒ Startup hatasÄ±: {e}")

@app.get("/", response_class=HTMLResponse)
async def serve_index():
    """Ana chat arayÃ¼zÃ¼nÃ¼ serve et"""
    return FileResponse("index.html")

@app.get("/health")
async def health_check():
    """Sistem saÄŸlÄ±k kontrolÃ¼"""
    uptime = datetime.utcnow() - connection_stats["start_time"]
    
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S"),
        "uptime_seconds": int(uptime.total_seconds()),
        "systems": {
            "gemini": "connected",
            "crewai": "async-ready",
            "langgraph": "async-active"
        },
        "stats": {
            "active_connections": connection_stats["active_connections"],
            "total_messages": connection_stats["messages_processed"],
            "crew_researches": connection_stats["crew_researches"],
            "async_operations": connection_stats["async_operations"]
        }
    }

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint - Async Gemini + CrewAI ile gerÃ§ek zamanlÄ± chat"""
    client_id = None
    
    try:
        await websocket.accept()
        client_id = id(websocket)
        
        # Connection stats update
        connection_stats["total_connections"] += 1
        connection_stats["active_connections"] += 1
        
        # Create dialog instance with websocket callback
        async def websocket_callback(message):
            try:
                await websocket.send_text(message)
                connection_stats["async_operations"] += 1
            except Exception as e:
                print(f"WebSocket callback error: {e}")
        
        dialog_instances[client_id] = AsyncLangGraphDialog(websocket_callback)
        
        print(f"ğŸ”Œ Yeni asenkron baÄŸlantÄ±: Client {client_id}")
        
        # Welcome message
        welcome_msg = {
            "type": "system",
            "content": "ğŸ¤– Async LangGraph + CrewAI Multi-Agent sistemine hoÅŸ geldiniz! Real-time progress tracking ile kapsamlÄ± araÅŸtÄ±rma yapabilirim.",
            "timestamp": datetime.utcnow().isoformat(),
            "stats": dialog_instances[client_id].get_conversation_stats()
        }
        await websocket.send_text(json.dumps(welcome_msg))
        
        while True:
            # Receive message
            data = await websocket.receive_text()
            message_data = json.loads(data)
            
            if "message" in message_data:
                user_message = message_data["message"]
                connection_stats["messages_processed"] += 1
                
                # CrewAI araÅŸtÄ±rmasÄ± mÄ± kontrol et
                if any(keyword in user_message.lower() for keyword in ["araÅŸtÄ±r", "research", "incele", "analiz"]):
                    connection_stats["crew_researches"] += 1
                
                print(f"ğŸ“¨ Async Client {client_id}: {user_message[:50]}...")
                
                try:
                    # ASENKRON olarak process et - UI donmayacak!
                    response = await dialog_instances[client_id].process_user_message(user_message)
                    
                    # Get conversation stats
                    stats = dialog_instances[client_id].get_conversation_stats()
                    
                    # Send response
                    response_data = {
                        "type": "message",
                        "content": response,
                        "timestamp": datetime.utcnow().isoformat(),
                        "intent": stats["current_intent"],
                        "stats": {
                            "total_messages": stats["total_messages"],
                            "intent": stats["current_intent"],
                            "crew_ai_enabled": stats["crew_ai_enabled"],
                            "has_research": stats["has_research_data"],
                            "async_mode": stats["async_mode"]
                        }
                    }
                    
                    await websocket.send_text(json.dumps(response_data))
                    print(f"âœ… Async Client {client_id}: YanÄ±t gÃ¶nderildi (Intent: {stats['current_intent']})")
                    
                except Exception as e:
                    # Error handling
                    error_response = {
                        "type": "error",
                        "content": f"ÃœzgÃ¼nÃ¼m, mesajÄ±nÄ±zÄ± iÅŸlerken bir hata oluÅŸtu: {str(e)}",
                        "timestamp": datetime.utcnow().isoformat(),
                        "error_code": "ASYNC_PROCESSING_ERROR"
                    }
                    
                    await websocket.send_text(json.dumps(error_response))
                    print(f"âŒ Async Client {client_id}: Hata - {str(e)}")
            
            elif message_data.get("type") == "ping":
                # Heartbeat response
                pong_data = {
                    "type": "pong",
                    "timestamp": datetime.utcnow().isoformat(),
                    "stats": connection_stats,
                    "async_mode": True
                }
                await websocket.send_text(json.dumps(pong_data))
    
    except WebSocketDisconnect:
        print(f"ğŸ”Œ Async Client {client_id} baÄŸlantÄ±sÄ± kesildi")
    
    except Exception as e:
        print(f"âŒ Async WebSocket hatasÄ± (Client {client_id}): {e}")
    
    finally:
        # Cleanup
        if client_id:
            connection_stats["active_connections"] -= 1
            
            if client_id in dialog_instances:
                del dialog_instances[client_id]
                print(f"ğŸ§¹ Async Client {client_id} temizlendi")

@app.get("/api/stats")
async def get_detailed_stats():
    """DetaylÄ± uygulama istatistikleri"""
    uptime = datetime.utcnow() - connection_stats["start_time"]
    
    return {
        "connections": {
            "active": connection_stats["active_connections"],
            "total": connection_stats["total_connections"]
        },
        "messages": {
            "total_processed": connection_stats["messages_processed"],
            "crew_researches": connection_stats["crew_researches"],
            "async_operations": connection_stats["async_operations"],
            "avg_per_connection": connection_stats["messages_processed"] / max(connection_stats["total_connections"], 1)
        },
        "uptime": {
            "seconds": int(uptime.total_seconds()),
            "formatted": str(uptime)
        },
        "system": {
            "gemini_model": Config.GEMINI_MODEL,
            "max_tokens": Config.GEMINI_MAX_TOKENS,
            "temperature": Config.GEMINI_TEMPERATURE,
            "features": ["Async LangGraph", "Async CrewAI", "Multi-Agent", "A2A Protocol", "Real-time Progress"],
            "mode": "FULL_ASYNC"
        }
    }

if __name__ == "__main__":
    print("ğŸŒŸ Async LangGraph + CrewAI Multi-Agent Chat System")
    print("=" * 60)
    print("ğŸ”‘ API AnahtarlarÄ±:")
    print(f"   âœ… Google API Key: {'âœ“' if Config.GOOGLE_API_KEY else 'âŒ'}")
    print(f"   ğŸ“Š Serper API Key: {'âœ“' if Config.SERPER_API_KEY else 'âŒ'}")
    print(f"   ğŸ¥ YouTube API Key: {'âœ“' if Config.YOUTUBE_API_KEY else 'âŒ'}")
    print(f"   ğŸ¦ Brave API Key: {'âœ“' if Config.BRAVE_API_KEY else 'âŒ'}")
    print("=" * 60)
    print("ğŸš€ Sistem Ã–zellikleri:")
    print("   âœ… FULL ASYNC LangGraph Conversation Flow")
    print("   âœ… ASYNC Gemini Pro Integration")
    print("   âœ… ASYNC CrewAI Multi-Agent System")
    print("   âœ… Real-time A2A Protocol Communication")
    print("   âœ… Non-blocking Progress Updates")
    print("   âœ… UI Responsive during LLM operations")
    print("   âœ… ThreadPoolExecutor for CrewAI")
    print("=" * 60)
    print("ğŸš€ BaÅŸlatÄ±lÄ±yor...")
    print("ğŸ“± Web UI: http://localhost:8000")
    print("ğŸ”— WebSocket: ws://localhost:8000/ws")
    print("ğŸ“Š Stats API: http://localhost:8000/api/stats")
    print("ğŸ’¡ Durdurmak iÃ§in: Ctrl+C")
    print("ğŸ’« UI artÄ±k LLM iÅŸlemleri sÄ±rasÄ±nda DONMAYACAK!")
    print("=" * 60)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )